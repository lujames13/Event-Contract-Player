# Task Spec G1.2 â€” è¶…åƒæ•¸èª¿å„ªã€MLP Baseline èˆ‡æ–‡ä»¶åŒæ­¥æ›´æ–°

<!-- status: draft -->
<!-- created: 2026-02-15 -->
<!-- architect: Claude Opus (Chat Project) -->

> **Gate:** 1ï¼ˆæ¨¡å‹å¯¦é©—æ± ï¼‰
> **å„ªå…ˆç´š:** ğŸŸ¡ High â€” Gate 1 æ ¸å¿ƒè¿­ä»£å·¥ä½œ
> **å‰ç½®æ¢ä»¶:** G1.1 å®Œæˆï¼ˆxgboost_v1/v2, lgbm_v1/v2 å·²æœ‰å›æ¸¬çµæœï¼‰

---

## ç›®æ¨™

1. **æ–‡ä»¶åŒæ­¥æ›´æ–°**ï¼šåæ˜ ä¸‰é …å·²ç¢ºèªçš„æ±ºç­–è®Šæ›´ï¼ˆ1440m æ’é™¤ã€æ ¡æº–è¦æ±‚å–æ¶ˆã€Gate 1 é€šéæ¢ä»¶ä¿®è¨‚ï¼‰
2. **Experiment 006**ï¼šå°ç›®å‰è¡¨ç¾æœ€ä½³çš„ lgbm 30m é€²è¡Œ Optuna è¶…åƒæ•¸èª¿å„ªï¼Œå˜—è©¦çªç ´ DA ç“¶é ¸
3. **Experiment 007**ï¼šå»ºç«‹ MLP (PyTorch) baselineï¼Œå¡«è£œ Gate 1 çš„ã€Œâ‰¥3 å·®ç•°åŒ–æ¶æ§‹ã€è¦æ±‚
4. **é™„å¸¶åˆ†æ**ï¼šæ¯è¼ªå¯¦é©—åŒæ™‚å ±å‘Šåè½‰ DA èˆ‡å„ fold ç©©å®šæ€§

---

## å­ä»»å‹™

### G1.2.0 â€” æ–‡ä»¶åŒæ­¥æ›´æ–°ï¼ˆå¿…é ˆæœ€å…ˆåŸ·è¡Œï¼‰

ä»¥ä¸‹ä¸‰é …æ±ºç­–è®Šæ›´å·²ç”±æ¶æ§‹å¸«èˆ‡ä½¿ç”¨è€…ç¢ºèªï¼Œéœ€åŒæ­¥åˆ°ç›¸é—œæ–‡ä»¶ï¼š

**è®Šæ›´ 1ï¼š1440m æ’é™¤æ–¼ Gate 1 è©•ä¼°**

- **`docs/PROGRESS.md`**ï¼š
  - Gate 1 é€šéæ¢ä»¶ä¸­ï¼Œå°‡ã€Œ4 å€‹ timeframeã€æ”¹ç‚ºã€Œ3 å€‹ timeframeï¼ˆ10m / 30m / 60mï¼‰ã€
  - æ–°å¢èªªæ˜ï¼šã€Œ1440m å› ç¾æœ‰è³‡æ–™é‡ä¸è¶³ä»¥æ”¯æ’æœ‰æ„ç¾©çš„ walk-forward é©—è­‰ï¼Œæš«æ™‚æ’é™¤ã€‚å¾…è³‡æ–™ç´¯ç© â‰¥ 3 å¹´å¾Œé‡æ–°è©•ä¼°ã€‚ã€
  - å›æ¸¬çµæœæ‘˜è¦è¡¨ä¸­ï¼Œç§»é™¤ 1440m ç›¸é—œçš„åˆ—
- **`docs/MODEL_ITERATIONS.md`**ï¼š
  - æ”¶æ–‚æ¨™æº–è¡¨ä¸­ï¼Œ1440m è¡ŒåŠ è¨»ã€Œâ¸ æš«åœã€
  - Scoreboard ä¸­ç§»é™¤ 1440m æ®µè½ï¼Œæ”¹ç‚ºä¸€è¡Œèªªæ˜ï¼šã€Œ1440m æš«åœè©•ä¼°ï¼Œè¦‹ PROGRESS.mdã€
  - Agent è‡ªä¸»è¿­ä»£è¦å‰‡ä¸­ï¼Œå›æ¸¬æŒ‡ä»¤æ”¹ç‚º `for tf in 10 30 60`

**è®Šæ›´ 2ï¼šæ ¡æº–è¦æ±‚å–æ¶ˆ**

- **`docs/PROGRESS.md`**ï¼š
  - Gate 1 é€šéæ¢ä»¶ä¸­ï¼Œåˆªé™¤ã€Œè©²çµ„åˆçš„ä¿¡å¿ƒåº¦æ ¡æº–ä¸åè½‰ï¼ˆé«˜ confidence bucket å‹ç‡ â‰¥ ä½ confidence bucketï¼‰ã€
  - æ›¿æ›ç‚ºï¼šã€Œè©²çµ„åˆçš„ OOS PnL > 0ï¼ˆåœ¨å›æ¸¬çš„æ¨¡æ“¬äº¤æ˜“ä¸­æ·¨ç›ˆåˆ©ï¼‰ã€
  - å®Œæ•´çš„ Gate 1 é€šéæ¢ä»¶æ‡‰æ”¹ç‚ºï¼š
    ```
    - [ ] â‰¥3 å€‹å·®ç•°åŒ–ç­–ç•¥æ¶æ§‹æœ‰å®Œæ•´å›æ¸¬æ•¸æ“š
    - [ ] æ¯å€‹ç­–ç•¥è¦†è“‹ 3 å€‹ timeframeï¼ˆ10m / 30m / 60mï¼‰
    - [ ] è‡³å°‘ 1 å€‹ã€Œç­–ç•¥ Ã— timeframeã€çµ„åˆ OOS DA > breakevenï¼ˆ10m: 55.56%, å…¶é¤˜: 54.05%ï¼‰
    - [ ] è©²çµ„åˆçš„ OOS PnL > 0ï¼ˆå›æ¸¬æ¨¡æ“¬äº¤æ˜“æ·¨ç›ˆåˆ©ï¼‰
    - [ ] è©²çµ„åˆ OOS äº¤æ˜“ç­†æ•¸ â‰¥ 500
    ```
- **`docs/MODEL_ITERATIONS.md`**ï¼š
  - æ”¶æ–‚æ¨™æº–æ®µè½ä¸­ï¼Œåˆªé™¤ã€Œä¿¡å¿ƒåº¦æ ¡æº–è¦æ±‚ã€æ•´æ®µ
  - æ›¿æ›ç‚ºï¼šã€Œ**PnL è¦æ±‚ï¼š** é”æ¨™çµ„åˆçš„ OOS æ¨¡æ“¬äº¤æ˜“ PnL å¿…é ˆ > 0ã€‚ã€
  - Scoreboard è¡¨æ ¼ä¸­ï¼Œã€Œæ ¡æº–ã€æ¬„ä½æ”¹ç‚ºã€ŒPnLã€æ¬„ä½ï¼ˆé¡¯ç¤ºæ­£è² ï¼‰
  - Agent åœæ­¢æ¢ä»¶ä¸­çš„ã€ŒæˆåŠŸã€å®šç¾©ï¼Œç§»é™¤æ ¡æº–ç›¸é—œæ¢ä»¶

**è®Šæ›´ 3ï¼šæ›´æ–° Scoreboard æ¬„ä½**

åŒæ­¥ Scoreboard è¡¨é ­ï¼Œå°‡ã€Œæ ¡æº–ã€æ¬„æ”¹ç‚ºã€ŒPnL âœ“ã€ï¼Œä»¥ âœ… / âŒ æ¨™ç¤º PnL æ­£è² ã€‚

**é©—æ”¶ï¼š**
1. `grep -c "1440" docs/PROGRESS.md` â€” ç¢ºèª 1440m ä¸å†å‡ºç¾åœ¨ Gate 1 é€šéæ¢ä»¶ä¸­
2. `grep -c "æ ¡æº–ä¸åè½‰" docs/PROGRESS.md` â€” è¿”å› 0
3. `grep -c "æ ¡æº–ä¸åè½‰" docs/MODEL_ITERATIONS.md` â€” è¿”å› 0
4. `grep "PnL > 0" docs/PROGRESS.md` â€” è‡³å°‘å‡ºç¾ä¸€æ¬¡

**ä¸è¦åšçš„äº‹ï¼š**
- ä¸è¦ä¿®æ”¹ `docs/DECISIONS.md`ï¼ˆæ ¡æº–é–¾å€¼ä¿ç•™ä½œç‚ºä¿¡å¿ƒåº¦éæ¿¾é–€æª»çš„åƒè€ƒï¼Œåªæ˜¯ä¸å†ä½œç‚º Gate 1 é€šéæ¢ä»¶ï¼‰
- ä¸è¦ä¿®æ”¹å·²å®Œæˆçš„å¯¦é©—è¨˜éŒ„å…§å®¹ï¼ˆExp 001-005 çš„çµæœä¿æŒåŸæ¨£ï¼‰
- ä¸è¦ä¿®æ”¹ `config/project_constants.yaml`

---

### G1.2.1 â€” Experiment 006: LightGBM Optuna è¶…åƒæ•¸èª¿å„ª

**å‡è¨­ï¼š** lgbm_v1 30m æ˜¯ç›®å‰å”¯ä¸€æ­£ PnL çµ„åˆï¼ˆDA 54.34%, PnL +14.93ï¼‰ï¼Œä½†ä½¿ç”¨çš„æ˜¯ LightGBM é è¨­åƒæ•¸ã€‚é€é Optuna è²è‘‰æ–¯æœç´¢æ‰¾åˆ°æ›´å¥½çš„è¶…åƒæ•¸çµ„åˆï¼Œé æœŸå¯ä»¥æå‡ DAã€‚

**å¯¦ä½œè¦æ±‚ï¼š**

1. **æ–°å»ºç­–ç•¥ `lgbm_v1_tuned`**ï¼ˆä¸ä¿®æ”¹ lgbm_v1ï¼‰ï¼š
   - ç›®éŒ„ï¼š`src/btc_predictor/strategies/lgbm_v1_tuned/`
   - å¾ `lgbm_v1` è¤‡è£½ `features.py`ï¼ˆç‰¹å¾µé›†ä¸è®Šï¼Œæ§åˆ¶å–®ä¸€è®Šæ•¸ï¼‰
   - æ–°å¢ `tuning.py`ï¼šOptuna èª¿å„ªé‚è¼¯
   - `strategy.py`ï¼šç¹¼æ‰¿ BaseStrategyï¼Œä½¿ç”¨èª¿å„ªå¾Œçš„æœ€ä½³åƒæ•¸

2. **Optuna æœç´¢ç©ºé–“ï¼š**
   ```python
   search_space = {
       "num_leaves": (15, 127),           # LightGBM æ ¸å¿ƒï¼Œæ§åˆ¶æ¨¡å‹è¤‡é›œåº¦
       "min_child_samples": (10, 100),     # é˜²æ­¢éæ“¬åˆçš„é—œéµ
       "learning_rate": (0.01, 0.3),       # log scale
       "subsample": (0.5, 1.0),            # è¡Œæ¡æ¨£
       "colsample_bytree": (0.3, 1.0),     # åˆ—æ¡æ¨£
       "reg_alpha": (1e-8, 10.0),          # L1 æ­£å‰‡åŒ–, log scale
       "reg_lambda": (1e-8, 10.0),         # L2 æ­£å‰‡åŒ–, log scale
       "early_stopping_rounds": (50, 150), # æœç´¢æœ€ä½³æ—©åœè¼ªæ•¸
   }
   ```

3. **Objective function è¨­è¨ˆï¼ˆé—œéµï¼‰ï¼š**
   - åœ¨ walk-forward æ¡†æ¶å…§è·‘ Optunaï¼Œ**ä¸æ˜¯**ç”¨ç°¡å–® train/test split
   - å…·é«”åšæ³•ï¼šå°æ¯å€‹ trial çš„è¶…åƒæ•¸çµ„åˆï¼Œè·‘ä¸€æ¬¡å®Œæ•´çš„ walk-forward backtestï¼ˆåƒ…é™ 30mï¼‰
   - Objective = OOS DAï¼ˆä¸»è¦æŒ‡æ¨™ï¼‰
   - **Seed stability**ï¼šæ¯å€‹ trial è·‘ 3 å€‹ random seedï¼ˆ`random_state` = 42, 123, 456ï¼‰ï¼Œå–å¹³å‡ DA ä½œç‚º objectiveã€‚é€™é˜²æ­¢å–®ä¸€ seed çš„éš¨æ©Ÿæ€§å½±éŸ¿æœç´¢æ–¹å‘ã€‚
   - æœç´¢æ¬¡æ•¸ **â‰¤ 50 trials**ï¼ˆ3 seeds Ã— 50 trials = 150 æ¬¡ walk-forwardï¼Œéœ€æ§åˆ¶è¨ˆç®—æ™‚é–“ï¼‰
   - å¦‚æœå–®æ¬¡ walk-forward å›æ¸¬è€—æ™‚éé•·ï¼ˆ> 10 åˆ†é˜ï¼‰ï¼Œå¯é™ä½ç‚º â‰¤ 30 trials

4. **è¨“ç·´åƒæ•¸ï¼š**
   - `train_days=180`ï¼ˆèˆ‡ lgbm_v1 ä¸€è‡´ï¼‰
   - `early_stopping_rounds` å¾æœç´¢ç©ºé–“å–å€¼ï¼ˆä¸å›ºå®š 50ï¼‰
   - Validation split: 20% with purged gapï¼ˆèˆ‡ xgboost_v2 ä¸€è‡´ï¼‰

5. **èª¿å„ªå®Œæˆå¾Œï¼š**
   - ç”¨æœ€ä½³åƒæ•¸è·‘ **10m / 30m / 60m** ä¸‰å€‹ timeframe çš„å®Œæ•´å›æ¸¬
   - è¨˜éŒ„æœ€ä½³åƒæ•¸çµ„åˆåˆ° Experiment 006 å€å¡Š
   - è¨˜éŒ„ Optuna æœç´¢çš„ top-5 trialsï¼ˆåƒæ•¸ + DAï¼‰ä¾›åƒè€ƒ

**é©—æ”¶ï¼š**
1. `src/btc_predictor/strategies/lgbm_v1_tuned/` ç›®éŒ„å­˜åœ¨ä¸”åŒ…å« `strategy.py`, `features.py`, `tuning.py`
2. `uv run pytest tests/test_strategies/test_lgbm_v1_tuned.py` é€šé
3. `docs/MODEL_ITERATIONS.md` æœ‰ Experiment 006 å®Œæ•´è¨˜éŒ„ï¼ˆå«æœ€ä½³åƒæ•¸ã€3 å€‹ TF çµæœã€top-5 trialsï¼‰
4. `reports/` ä¸‹æœ‰ 3 å€‹æ–°çš„å›æ¸¬å ±å‘Š JSONï¼ˆlgbm_v1_tuned çš„ 10m/30m/60mï¼‰

**ä¸è¦åšçš„äº‹ï¼š**
- ä¸è¦ä¿®æ”¹ `lgbm_v1` çš„ä»»ä½•æª”æ¡ˆï¼ˆé€™æ˜¯å°ç…§çµ„ï¼‰
- ä¸è¦åœ¨ Optuna ä¸­æœç´¢ç‰¹å¾µé›†ï¼ˆæœ¬æ¬¡æ§åˆ¶ç‰¹å¾µä¸è®Šï¼‰
- ä¸è¦è¶…é 50 trialsï¼ˆè¨ˆç®—è³‡æºæœ‰é™ï¼‰
- ä¸è¦ç”¨ Optuna çš„ multi-objective æ¨¡å¼
- ä¸è¦åœ¨ 30m ä»¥å¤–çš„ timeframe ä¸Šè·‘ Optunaï¼ˆå…¶ä»– TF åªç”¨ 30m çš„æœ€ä½³åƒæ•¸è·‘å›æ¸¬ï¼‰

---

### G1.2.2 â€” Experiment 007: Simple MLP Baseline (PyTorch)

**å‡è¨­ï¼š** Neural network å¯èƒ½æ•æ‰ tree-based models éºæ¼çš„éç·šæ€§äº¤äº’æ•ˆæ‡‰ã€‚MLP æ˜¯æœ€ä½æˆæœ¬çš„ neural baselineï¼Œç”¨ä¾†åˆ¤æ–·æ˜¯å¦å€¼å¾—æŠ•å…¥æ›´è¤‡é›œçš„æ¶æ§‹ï¼ˆå¦‚ N-BEATSï¼‰ã€‚

**å¯¦ä½œè¦æ±‚ï¼š**

1. **æ–°å»ºç­–ç•¥ `mlp_v1`**ï¼š
   - ç›®éŒ„ï¼š`src/btc_predictor/strategies/mlp_v1/`
   - `features.py`ï¼šå¾ `lgbm_v1/features.py` è¤‡è£½ï¼Œä½†æ–°å¢ **rolling z-score normalization**
     - é‡è¦ï¼šz-score å¿…é ˆæ˜¯ rolling windowï¼ˆå¦‚ past 60 barsï¼‰ï¼Œä¸å¯ç”¨å…¨å±€çµ±è¨ˆé‡ï¼Œé¿å… look-ahead bias
     - å…¬å¼ï¼š`z = (x - rolling_mean(x, 60)) / rolling_std(x, 60)`
   - `model.py`ï¼šPyTorch MLP å®šç¾©èˆ‡è¨“ç·´é‚è¼¯
   - `strategy.py`ï¼šç¹¼æ‰¿ BaseStrategy

2. **MLP æ¶æ§‹ï¼š**
   ```
   Input (N features)
     â†’ Linear(N, 128) â†’ BatchNorm â†’ ReLU â†’ Dropout(0.3)
     â†’ Linear(128, 64) â†’ BatchNorm â†’ ReLU â†’ Dropout(0.3)
     â†’ Linear(64, 1) â†’ Sigmoid
   ```
   - Loss: `BCELoss`
   - Optimizer: `Adam(lr=1e-3, weight_decay=1e-5)`
   - Epochs: 50ï¼Œwith early stopping (patience=10) on validation loss
   - Batch size: 512
   - Validation split: 20% with purged gap

3. **Input è¨­è¨ˆï¼š**
   - ä¸ä½¿ç”¨åºåˆ—è¼¸å…¥ï¼ˆé‚£æ˜¯ RNN/N-BEATS çš„äº‹ï¼‰
   - ç›´æ¥ä½¿ç”¨ t æ™‚åˆ»çš„ feature vectorï¼ˆè·Ÿ tree models ä¸€è‡´ï¼‰
   - é€™ç¢ºä¿æˆ‘å€‘åœ¨æ¯”è¼ƒ MLP vs tree æ™‚ï¼Œå”¯ä¸€çš„è®Šæ•¸æ˜¯æ¨¡å‹æ¶æ§‹

4. **è¨“ç·´åƒæ•¸ï¼š**
   - `train_days=180`
   - ä½¿ç”¨ GPUï¼ˆ`torch.device('cuda' if torch.cuda.is_available() else 'cpu')`ï¼‰
   - æ¨¡å‹åºåˆ—åŒ–ï¼š`torch.save(model.state_dict(), path)`

5. **è·‘ 10m / 30m / 60m ä¸‰å€‹ timeframe çš„å®Œæ•´å›æ¸¬**

**é©—æ”¶ï¼š**
1. `src/btc_predictor/strategies/mlp_v1/` ç›®éŒ„å­˜åœ¨ä¸”åŒ…å« `strategy.py`, `features.py`, `model.py`
2. `uv run pytest tests/test_strategies/test_mlp_v1.py` é€šé
3. `uv run python scripts/train_model.py --strategy mlp_v1 --timeframe 30` å¯æ­£å¸¸åŸ·è¡Œ
4. `docs/MODEL_ITERATIONS.md` æœ‰ Experiment 007 å®Œæ•´è¨˜éŒ„
5. `reports/` ä¸‹æœ‰ 3 å€‹æ–°çš„å›æ¸¬å ±å‘Š JSON
6. æ¨ç†å»¶é² < 1 ç§’ï¼ˆå–®æ¬¡ predict å‘¼å«ï¼‰

**ä¸è¦åšçš„äº‹ï¼š**
- ä¸è¦ä½¿ç”¨åºåˆ—æ¨¡å‹ï¼ˆRNN, LSTM, Transformerï¼‰â€” é‚£æ˜¯ä¸åŒçš„æ¶æ§‹é¡åˆ¥
- ä¸è¦ä½¿ç”¨å…¨å±€ normalizationï¼ˆå¿…é ˆæ˜¯ rolling z-scoreï¼‰
- ä¸è¦å˜—è©¦è¶…é 3 å±¤çš„ MLPï¼ˆä¿æŒç°¡å–®ï¼Œé€™æ˜¯ baselineï¼‰
- ä¸è¦å¼•å…¥ `torchvision` æˆ–å…¶ä»–ä¸å¿…è¦çš„ PyTorch å­å¥—ä»¶
- ä¸è¦ä½¿ç”¨ > 4GB VRAM çš„é…ç½®ï¼ˆbatch_size å’Œ hidden_dim è¦ä¿å®ˆï¼‰

---

### G1.2.3 â€” é™„å¸¶åˆ†æï¼šåè½‰ DA èˆ‡ Fold ç©©å®šæ€§

**èƒŒæ™¯ï¼š** ä½¿ç”¨è€…æå‡ºã€Œåè½‰éæ“¬åˆæ¨¡å‹ã€çš„å‡è¨­ã€‚éœ€è¦é›¶æˆæœ¬é©—è­‰ã€‚

**å¯¦ä½œè¦æ±‚ï¼š**

åœ¨ `src/btc_predictor/backtest/stats.py` çš„ `calculate_backtest_stats()` ä¸­æ–°å¢å…©å€‹æ¬„ä½ï¼š

1. **`inverted_da`**ï¼š`1.0 - total_da`ï¼Œåè½‰å¾Œçš„ç†è«– DA
2. **`per_fold_da`**ï¼šä¸€å€‹ listï¼Œè¨˜éŒ„æ¯å€‹ walk-forward fold çš„ç¨ç«‹ DA

å…·é«”åšæ³•ï¼š
- åœ¨ `run_backtest()` çš„å›å‚³å€¼ä¸­ï¼Œtrades å·²ç¶“åŒ…å« `open_time`ã€‚æ ¹æ“š walk-forward çš„ `test_days` é‚Šç•Œï¼Œå°‡ trades åˆ†çµ„åˆ°å„ foldã€‚
- è¨ˆç®—æ¯å€‹ fold çš„ DAï¼Œå­˜ç‚º listã€‚
- åœ¨ backtest report JSON ä¸­æ–°å¢ `"inverted_da"` å’Œ `"per_fold_da"` æ¬„ä½ã€‚

**MODEL_ITERATIONS.md æ›´æ–°ï¼š** å¾ Experiment 006 èµ·ï¼Œçµæœè¡¨æ ¼æ–°å¢å…©æ¬„ï¼š
- `Inv. DA`ï¼šåè½‰ DA
- `Fold Ïƒ`ï¼šå„ fold DA çš„æ¨™æº–å·®ï¼ˆç©©å®šæ€§æŒ‡æ¨™ï¼Œè¶Šä½è¶Šå¥½ï¼‰

**é©—æ”¶ï¼š**
1. `uv run pytest tests/test_backtest_stats.py` é€šéï¼ˆå«æ–°æ¬„ä½çš„æ¸¬è©¦ï¼‰
2. å›æ¸¬å ±å‘Š JSON åŒ…å« `inverted_da` å’Œ `per_fold_da` æ¬„ä½
3. Experiment 006 å’Œ 007 çš„çµæœè¡¨æ ¼æœ‰ `Inv. DA` å’Œ `Fold Ïƒ` æ¬„

**ä¸è¦åšçš„äº‹ï¼š**
- ä¸è¦ä¿®æ”¹ `run_backtest()` çš„å‡½æ•¸ç°½åæˆ–æ ¸å¿ƒé‚è¼¯
- ä¸è¦å»ºç«‹ç¨ç«‹çš„åè½‰ç­–ç•¥æ¨¡å‹ï¼ˆåªæ˜¯åœ¨çµ±è¨ˆå ±å‘Šä¸­åŠ ä¸€å€‹è¨ˆç®—å€¼ï¼‰

---

## åŸ·è¡Œé †åº

```
G1.2.0ï¼ˆæ–‡ä»¶æ›´æ–°ï¼‰â€” å¿…é ˆæœ€å…ˆï¼Œå¾ŒçºŒå¯¦é©—ä¾è³´æ–°çš„æ”¶æ–‚æ¨™æº–
  â†“
G1.2.3ï¼ˆstats æ“´å±•ï¼‰â€” å¾ŒçºŒå¯¦é©—éœ€è¦ç”¨åˆ°æ–°æ¬„ä½
  â†“
G1.2.1ï¼ˆExp 006: Optunaï¼‰â€” è¨ˆç®—é‡æœ€å¤§ï¼Œå…ˆè·‘
  â†“
G1.2.2ï¼ˆExp 007: MLPï¼‰â€” èˆ‡ Optuna çµæœç„¡ä¾è³´
```

---

## ä¿®æ”¹ç¯„åœï¼ˆå°é–‰æ¸…å–®ï¼‰

**æ–°å¢ï¼š**
- `src/btc_predictor/strategies/lgbm_v1_tuned/` â€” æ•´å€‹ç›®éŒ„ï¼ˆ`__init__.py`, `strategy.py`, `features.py`, `tuning.py`ï¼‰
- `src/btc_predictor/strategies/mlp_v1/` â€” æ•´å€‹ç›®éŒ„ï¼ˆ`__init__.py`, `strategy.py`, `features.py`, `model.py`ï¼‰
- `tests/test_strategies/test_lgbm_v1_tuned.py`
- `tests/test_strategies/test_mlp_v1.py`

**ä¿®æ”¹ï¼š**
- `docs/PROGRESS.md` â€” Gate 1 é€šéæ¢ä»¶ã€å›æ¸¬çµæœæ‘˜è¦è¡¨
- `docs/MODEL_ITERATIONS.md` â€” æ”¶æ–‚æ¨™æº–ã€Scoreboardã€æ–°å¢ Exp 006/007ã€åœæ­¢æ¢ä»¶æ›´æ–°
- `src/btc_predictor/backtest/stats.py` â€” æ–°å¢ `inverted_da` å’Œ `per_fold_da`
- `tests/test_backtest_stats.py` â€” æ–°å¢å°æ‡‰æ¸¬è©¦

**ä¸å‹•ï¼š**
- `docs/DECISIONS.md`
- `config/project_constants.yaml`
- `src/btc_predictor/strategies/base.py`
- `src/btc_predictor/backtest/engine.py`
- `src/btc_predictor/simulation/risk.py`
- `src/btc_predictor/strategies/xgboost_v1/`
- `src/btc_predictor/strategies/xgboost_v2/`
- `src/btc_predictor/strategies/lgbm_v1/`
- `src/btc_predictor/strategies/lgbm_v2/`

---

## åœæ­¢æ¢ä»¶

**æœ¬ task spec æ˜¯æœ‰é™ç¯„åœä»»å‹™ï¼ˆExp 006 + 007ï¼‰ï¼Œä¸æ˜¯ç„¡é™è‡ªä¸»è¿­ä»£ã€‚**

å®Œæˆ G1.2.0 â†’ G1.2.3 â†’ G1.2.1 â†’ G1.2.2 å¾Œï¼Œç„¡è«–çµæœå¦‚ä½•ï¼Œåœä¸‹ç­‰æ¶æ§‹å¸« review å†æ±ºå®šä¸‹ä¸€æ­¥ã€‚

æˆåŠŸåˆ¤å®šæ¨™æº–ï¼ˆç”¨æ–¼ MODEL_ITERATIONS.md Scoreboard æ¨™è¨˜ï¼‰ï¼š
- âœ… **é”æ¨™**ï¼šOOS DA > breakeven + OOS PnL > 0 + äº¤æ˜“ç­†æ•¸ â‰¥ 500
- âŒ **æœªé”æ¨™**ï¼šä»»ä¸€æ¢ä»¶ä¸æ»¿è¶³

---

## é©—æ”¶æ¨™æº–ï¼ˆæŒ‰é †åºåŸ·è¡Œï¼‰

```bash
# 0. æ–‡ä»¶æ›´æ–°é©—è­‰
grep "PnL > 0" docs/PROGRESS.md                        # æ‡‰å‡ºç¾
grep -c "æ ¡æº–ä¸åè½‰" docs/MODEL_ITERATIONS.md            # æ‡‰è¿”å› 0

# 1. æ‰€æœ‰æ¸¬è©¦é€šé
uv run pytest

# 2. æ–°ç­–ç•¥å¯è¢« Registry ç™¼ç¾
uv run python -c "
from pathlib import Path
from btc_predictor.strategies.registry import StrategyRegistry
reg = StrategyRegistry()
reg.discover(Path('src/btc_predictor/strategies'), Path('models'))
names = reg.list_names()
assert 'lgbm_v1_tuned' in names, f'lgbm_v1_tuned not found in {names}'
assert 'mlp_v1' in names, f'mlp_v1 not found in {names}'
print('âœ… Registry discovers both new strategies')
"

# 3. å›æ¸¬å ±å‘ŠåŒ…å«æ–°æ¬„ä½
uv run python -c "
import json, glob
reports = glob.glob('reports/backtest_lgbm_v1_tuned_*.json')
assert len(reports) >= 3, f'Expected 3+ reports, found {len(reports)}'
with open(reports[0]) as f:
    data = json.load(f)
assert 'inverted_da' in data['stats'], 'Missing inverted_da'
assert 'per_fold_da' in data['stats'], 'Missing per_fold_da'
print('âœ… New stats fields present')
"

# 4. MODEL_ITERATIONS.md æœ‰ Exp 006 å’Œ 007
grep "Experiment 006" docs/MODEL_ITERATIONS.md
grep "Experiment 007" docs/MODEL_ITERATIONS.md
```

---

## Coding Agent å›å ±å€

### å¯¦ä½œçµæœ
<!-- å®Œæˆäº†ä»€éº¼ï¼Œä¿®æ”¹äº†å“ªäº›æª”æ¡ˆ -->

### é©—æ”¶è‡ªæª¢
<!-- é€æ¢åˆ—å‡ºé©—æ”¶æ¨™æº–çš„ pass/fail -->

### é‡åˆ°çš„å•é¡Œ
<!-- æŠ€è¡“éšœç¤™ã€è¨­è¨ˆç–‘æ…® -->

### PROGRESS.md ä¿®æ”¹å»ºè­°
<!-- å¦‚æœå¯¦ä½œéç¨‹ä¸­ç™¼ç¾è¦åŠƒéœ€è¦èª¿æ•´ï¼Œåœ¨æ­¤èªªæ˜ -->

---

## Review Agent å›å ±å€

### å¯©æ ¸çµæœï¼š[PASS / FAIL / PASS WITH NOTES]

### é©—æ”¶æ¨™æº–æª¢æŸ¥
<!-- é€æ¢ âœ…/âŒ -->

### ä¿®æ”¹ç¯„åœæª¢æŸ¥
<!-- git diff --name-only çš„çµæœæ˜¯å¦åœ¨ç¯„åœå…§ -->

### ç™¼ç¾çš„å•é¡Œ
<!-- å…·é«”å•é¡Œæè¿° -->

### PROGRESS.md ä¿®æ”¹å»ºè­°
<!-- å¦‚æœ‰ -->